{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9ed3a5-232f-4143-853e-fe656c3a6aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ac8c99-5742-4362-a2f9-42aa59f8254c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AudioData', 'AudioDataFormat', 'BoundingBox', 'Category', 'ClassificationResult', 'Classifications', 'Detection', 'DetectionResult', 'Embedding', 'EmbeddingResult', 'Landmark', 'LandmarksDetectionResult', 'NormalizedLandmark', 'NormalizedRect', 'Rect', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'keypoint']\n",
      "['__annotations__', '__class__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'create_from_pb2', 'to_pb2']\n"
     ]
    }
   ],
   "source": [
    "print(dir(mp.tasks.components.containers))\n",
    "print(dir(mp.tasks.components.containers.DetectionResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae4957-670d-47d7-9a39-7bbda8069b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af4e0300-fe3e-4b04-972d-b2f30d4e4781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct imports using top-level aliases\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "DetectionResult = mp.tasks.components.containers.DetectionResult\n",
    "ObjectDetector = mp.tasks.vision.ObjectDetector\n",
    "ObjectDetectorOptions = mp.tasks.vision.ObjectDetectorOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05393fea-a380-43a3-847a-edbc6e1f184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… Replace this with the path to your .tflite model\n",
    "model_path = \"efficientdet_lite0-2.tflite\"  # or any object detection TFLite model\n",
    "import time\n",
    "import os\n",
    "assert os.path.exists(model_path), f\"Model file not found at {model_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5776cf-0791-4369-9a21-afcb79a2917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import streaming_utils_Tut\n",
    "streaming_utils_Tut.getStream(0)\n",
    "\n",
    "cap = streaming_utils_Tut.getStream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c5b94d-1f11-4a38-a15c-e2041e2ed3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Callback for handling results\n",
    "def detection_callback(result: DetectionResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    print(\"function?\")\n",
    "    annotated_image = output_image.numpy_view()\n",
    "\n",
    "    for detection in result.detections:\n",
    "        bbox = detection.bounding_box\n",
    "        start_point = (int(bbox.origin_x), int(bbox.origin_y))\n",
    "        end_point = (\n",
    "            int(bbox.origin_x + bbox.width),\n",
    "            int(bbox.origin_y + bbox.height),\n",
    "        )\n",
    "        cv2.rectangle(annotated_image, start_point, end_point, (0, 255, 0), 2)\n",
    "\n",
    "        category = detection.categories[0]\n",
    "        label = f\"{category.category_name}: {int(category.score * 100)}%\"\n",
    "        cv2.putText(\n",
    "            annotated_image,\n",
    "            label,\n",
    "            (start_point[0], start_point[1] - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            (0, 255, 0),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    cv2.imshow(\"MediaPipe Live Stream Object Detection\", annotated_image)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to quit\n",
    "        return True  # signal to stop streaming\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0cbdf0a-9361-418b-877d-e63b3c1cd210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751384258.936243 4367197 gl_context.cc:357] GL version: 2.1 (2.1 ATI-7.0.3), renderer: AMD Radeon Pro 560X OpenGL Engine\n",
      "/opt/anaconda3/envs/Full_mp_environ/lib/python3.12/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press ESC to exit\n"
     ]
    }
   ],
   "source": [
    "# Set up detector options\n",
    "\n",
    "#base_options = BaseOptions(model_asset_path=model_path)\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "ObjectDetectorOptions = mp.tasks.vision.ObjectDetectorOptions\n",
    "ObjectDetector = mp.tasks.vision.ObjectDetector\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "DetectionResult = mp.tasks.components.containers.DetectionResult\n",
    "\n",
    "# Create detector in IMAGE mode\n",
    "options = ObjectDetectorOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    score_threshold=0.5,\n",
    ")\n",
    "\n",
    "detector = ObjectDetector.create_from_options(options)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Wrap frame in MediaPipe Image\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "    # Detect objects (synchronously)\n",
    "    result = detector.detect(mp_image)\n",
    "\n",
    "    # Draw only person detections\n",
    "    for detection in result.detections:\n",
    "        category = detection.categories[0]\n",
    "        if category.category_name == \"person\":\n",
    "            bbox = detection.bounding_box\n",
    "            start = (int(bbox.origin_x), int(bbox.origin_y))\n",
    "            end = (int(bbox.origin_x + bbox.width), int(bbox.origin_y + bbox.height))\n",
    "            cv2.rectangle(frame, start, end, (0, 255, 0), 2)\n",
    "            label = f\"{category.category_name}: {category.score:.2f}\"\n",
    "            cv2.putText(frame, label, (start[0], start[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Person Detection (IMAGE mode)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC key\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058489b-3846-41bc-a934-4ab934a2a102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
